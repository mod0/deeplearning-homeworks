import numpy as np
import homework2q1b as hw2

###
# Dropouts with Relu Activation
###

#########################################################################
# Run the stochastic gradient descent algorithm with dropouts
#########################################################################

params = hw2.initialize(eta = 0.01, maxiter = 20, early_termination = False)
model  = hw2.sgd(**params)


#########################################################################
# Validate using Keras model
#########################################################################

from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.optimizers import SGD

x_train     = params['train_data'][:,:-1]
y_train     = np.array([hw2.label_to_ytrue(2, x) for x in params['train_data'][:, -1]])
x_test      = params['test_data'][:,:-1]
y_test      = np.array([hw2.label_to_ytrue(2, x) for x in params['test_data'][:, -1]])

keras_model = Sequential()
keras_model.add(Dense(3, activation='relu', use_bias = False, kernel_initializer='random_normal', input_dim = 2))
keras_model.add(Dense(3, activation='relu', use_bias = False, kernel_initializer='random_normal'))
keras_model.add(Dropout(1.0/3))
keras_model.add(Dense(2, activation='softmax', use_bias = False, kernel_initializer='random_normal'))
sgd = SGD(lr=params['eta'])
keras_model.compile(optimizer=sgd, loss='mean_squared_error', metrics=['accuracy'])
keras_model.fit(x_train, y_train, batch_size = 1, epochs=params['maxiter'])
score = keras_model.evaluate(x_test, y_test, batch_size=1)
kw = keras_model.get_weights()


######################################################################
# Display statistics
######################################################################

print """
##############################################################
Weights corresponding to best testing accuracy (custom SGD): 
##############################################################"""
print "W1: ", model['w'][0]
print "W2: ", model['w'][1]
print "W3: ", model['w'][2]

# Note: In the following plots, we show how 
# training and testing loss changes with EPOCH.
# SGD algorithm is terminated once testing loss begins 
# to increase again
# hw2.doplots("Loss vs Epochs (training)", model['total_training_loss_fn_value'], "Training Loss", "Epochs", "Loss", semilogy = True)
# hw2.doplots("Loss vs Epochs (testing)", model['total_testing_loss_fn_value'], "Testing Loss", "Epochs", "Loss", semilogy = True)

print """
##############################################################
Weights obtained from Keras: 
##############################################################"""
print "Keras W1:", kw[0]
print "Keras W2:", kw[1]
print "Keras W3:", kw[2]

# Keras multiplies by weight vectors from the right so transposing it.
kwt = []
kwt.append(kw[0].T)
kwt.append(kw[1].T)
kwt.append(kw[2].T)

# None of the units will drop out hence we use a 0 probability for dropout
testing_mask = hw2.generate_masks_for_layers(3, [2, 3, 3], [0, 0, 0])

# Find the f_score and other metrics
precision_recall_fscore = hw2.get_precision_recall_fscore(params['shuffle_order_testing'], params['test_data'], model['w'], params['z'], params['a'], params['y'], testing_mask)
print """
#########################################################################
Precision, Recall and F-score for model output by running custom SGD
#########################################################################
"""
print "Precision: ", precision_recall_fscore[0]
print "Recall: ", precision_recall_fscore[1]
print "F-Score: ", precision_recall_fscore[2]

# Find the f_score and other metrics
precision_recall_fscore = hw2.get_precision_recall_fscore(params['shuffle_order_testing'], params['test_data'], kwt, params['z'], params['a'], params['y'], testing_mask)
print """
#########################################################################
Precision, Recall and F-score for model generated by running Keras SGD
#########################################################################
"""
print "Precision: ", precision_recall_fscore[0]
print "Recall: ", precision_recall_fscore[1]
print "F-Score: ", precision_recall_fscore[2]
