{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import homework2q1c as hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data of shape: (1000, 3)<type 'numpy.ndarray'>\n",
      "Loaded testing data of shape: (1000, 3)<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# Run the stochastic gradient descent algorithm with dropouts\n",
    "#######################################################################\n",
    "trainfile = 'train_data.txt'\n",
    "testfile  = 'test_data.txt'\n",
    "params = hw2.initialize(trainfile, testfile, eta = 0.001, maxiter = 20, \n",
    "                        early_termination = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent: Running epoch 0\n",
      "Loss: 0.140981651595                    \n",
      "\n",
      "Total training loss in this epoch: 595.956189081\n",
      "Total testing loss in this epoch: 575.684559471\n",
      "Stochastic Gradient Descent: Running epoch 1\n",
      "Loss: 1.68655286667                    \n",
      "\n",
      "Total training loss in this epoch: 318.85249165\n",
      "Total testing loss in this epoch: 298.593448118\n",
      "Stochastic Gradient Descent: Running epoch 2\n",
      "Loss: 1.03353864318                    \n",
      "\n",
      "Total training loss in this epoch: 235.996757015\n",
      "Total testing loss in this epoch: 240.899612726\n",
      "Stochastic Gradient Descent: Running epoch 3\n",
      "Loss: 0.0457441393006                    \n",
      "\n",
      "Total training loss in this epoch: 221.283178831\n",
      "Total testing loss in this epoch: 228.908218321\n",
      "Stochastic Gradient Descent: Running epoch 4\n",
      "Loss: 0.03556922832                    \n",
      "\n",
      "Total training loss in this epoch: 204.240332123\n",
      "Total testing loss in this epoch: 211.471425\n",
      "Stochastic Gradient Descent: Running epoch 5\n",
      "Loss: 0.0202380919094                    \n",
      "\n",
      "Total training loss in this epoch: 180.534660142\n",
      "Total testing loss in this epoch: 187.768085961\n",
      "Stochastic Gradient Descent: Running epoch 6\n",
      "Loss: 0.5                    \n",
      "\n",
      "Total training loss in this epoch: 149.997248195\n",
      "Total testing loss in this epoch: 156.964597992\n",
      "Stochastic Gradient Descent: Running epoch 7\n",
      "Loss: 0.5                    \n",
      "\n",
      "Total training loss in this epoch: 121.123567593\n",
      "Total testing loss in this epoch: 127.667053934\n",
      "Stochastic Gradient Descent: Running epoch 8\n",
      "Loss: 0.5                    \n",
      "\n",
      "Total training loss in this epoch: 94.4592410196\n",
      "Total testing loss in this epoch: 101.176131927\n",
      "Stochastic Gradient Descent: Running epoch 9\n",
      "Loss: 0.580474299295                    \n",
      "\n",
      "Total training loss in this epoch: 71.6740536851\n",
      "Total testing loss in this epoch: 78.5063352702\n",
      "Stochastic Gradient Descent: Running epoch 10\n",
      "Loss: 0.000110952803161                    \n",
      "\n",
      "Total training loss in this epoch: 54.8124448656\n",
      "Total testing loss in this epoch: 61.1887035289\n",
      "Stochastic Gradient Descent: Running epoch 11\n",
      "Loss: 4.97281201533e-05                    \n",
      "\n",
      "Total training loss in this epoch: 42.458372851\n",
      "Total testing loss in this epoch: 47.5643617545\n",
      "Stochastic Gradient Descent: Running epoch 12\n",
      "Loss: 0.5                    \n",
      "\n",
      "Total training loss in this epoch: 33.4675307766\n",
      "Total testing loss in this epoch: 37.1351723697\n",
      "Stochastic Gradient Descent: Running epoch 13\n",
      "Loss: 2.21263721716e-05                    \n",
      "\n",
      "Total training loss in this epoch: 29.7512832483\n",
      "Total testing loss in this epoch: 32.2943881509\n",
      "Stochastic Gradient Descent: Running epoch 14\n",
      "Loss: 1.6522060883e-05                    \n",
      "\n",
      "Total training loss in this epoch: 26.2152200541\n",
      "Total testing loss in this epoch: 27.9556261505\n",
      "Stochastic Gradient Descent: Running epoch 15\n",
      "Loss: 8.60466895807e-06                    \n",
      "\n",
      "Total training loss in this epoch: 22.167570785\n",
      "Total testing loss in this epoch: 23.3254682765\n",
      "Stochastic Gradient Descent: Running epoch 16\n",
      "Loss: 7.74426710248e-06                    \n",
      "\n",
      "Total training loss in this epoch: 20.2352953708\n",
      "Total testing loss in this epoch: 21.0841232084\n",
      "Stochastic Gradient Descent: Running epoch 17\n",
      "Loss: 0.5                    \n",
      "\n",
      "Total training loss in this epoch: 17.8242741897\n",
      "Total testing loss in this epoch: 18.396964918\n",
      "Stochastic Gradient Descent: Running epoch 18\n",
      "Loss: 2.83788651693e-06                    \n",
      "\n",
      "Total training loss in this epoch: 15.6503641138\n",
      "Total testing loss in this epoch: 16.0171291037\n",
      "Stochastic Gradient Descent: Running epoch 19\n",
      "Loss: 2.54849798116e-06                    \n",
      "\n",
      "Total training loss in this epoch: 14.4788865972\n",
      "Total testing loss in this epoch: 14.7769023572\n",
      "Stochastic Gradient Descent: Running epoch 20\n",
      "Loss: 1.39488337878e-06                    \n",
      "\n",
      "Total training loss in this epoch: 13.0119346313\n",
      "Total testing loss in this epoch: 13.1740704892\n",
      "Stochastic Gradient Descent: Running epoch 21\n",
      "Loss: 1.17709491957e-06                    \n",
      "\n",
      "Total training loss in this epoch: 12.1128365116\n",
      "Total testing loss in this epoch: 12.1608793215\n",
      "Stochastic Gradient Descent: Running epoch 22\n",
      "Loss: 1.03788305915e-06                    \n",
      "\n",
      "Total training loss in this epoch: 11.4024555424\n",
      "Total testing loss in this epoch: 11.3483871126\n",
      "Stochastic Gradient Descent: Running epoch 23\n",
      "Loss: 7.4339502204e-07                    \n",
      "\n",
      "Total training loss in this epoch: 10.4895003687\n",
      "Total testing loss in this epoch: 10.3274936632\n",
      "Stochastic Gradient Descent: Running epoch 24\n",
      "Loss: 0.5                    \n",
      "\n",
      "Total training loss in this epoch: 10.1504404758\n",
      "Total testing loss in this epoch: 9.89537593914\n",
      "Stochastic Gradient Descent: Running epoch 25\n",
      "Loss: 3.85898372947e-07                    \n",
      "\n",
      "Total training loss in this epoch: 9.25750085425\n",
      "Total testing loss in this epoch: 8.92017737691\n",
      "Stochastic Gradient Descent: Running epoch 26\n",
      "Loss: 0.5                    \n",
      "\n",
      "Total training loss in this epoch: 8.70180269377\n",
      "Total testing loss in this epoch: 8.31046621405\n",
      "Stochastic Gradient Descent: Running epoch 27\n",
      "Loss: 2.1898461659e-07                    \n",
      "\n",
      "Total training loss in this epoch: 8.22387178369\n",
      "Total testing loss in this epoch: 7.78857580611\n",
      "Stochastic Gradient Descent: Running epoch 28\n",
      "Loss: 1.60409639464e-07                    \n",
      "\n",
      "Total training loss in this epoch: 7.9042407285\n",
      "Total testing loss in this epoch: 7.41970149425\n",
      "Stochastic Gradient Descent: Running epoch 29\n",
      "Loss: 1.00891798149e-07                    \n",
      "\n",
      "Total training loss in this epoch: 7.52890245196\n",
      "Total testing loss in this epoch: 7.01029041396\n",
      "Stochastic Gradient Descent: Running epoch 30\n",
      "Loss: 6.04037369457e-08                    \n",
      "\n",
      "Total training loss in this epoch: 7.31496044939\n",
      "Total testing loss in this epoch: 6.78742527568\n",
      "Stochastic Gradient Descent: Running epoch 31\n",
      "Loss: 4.11241579178e-08                    \n",
      "\n",
      "Total training loss in this epoch: 6.71081686236\n",
      "Total testing loss in this epoch: 6.20170785043\n",
      "Stochastic Gradient Descent: Running epoch 32\n",
      "Loss: 0.5                    \n",
      "\n",
      "Total training loss in this epoch: 6.77536150729\n",
      "Total testing loss in this epoch: 6.26466933358\n",
      "Stochastic Gradient Descent: Running epoch 33\n",
      "Loss: 2.14549123997e-08                    \n",
      "\n",
      "Total training loss in this epoch: 6.21774306558\n",
      "Total testing loss in this epoch: 5.72723784926\n",
      "Stochastic Gradient Descent: Running epoch 34\n",
      "Loss: 4.28164455135e-08                    \n",
      "\n",
      "Total training loss in this epoch: 6.26934434049\n",
      "Total testing loss in this epoch: 5.77933490597\n",
      "Stochastic Gradient Descent: Running epoch 35\n",
      "Loss: 0.5                    \n",
      "\n",
      "Total training loss in this epoch: 6.06862612911\n",
      "Total testing loss in this epoch: 5.59563378745\n",
      "Stochastic Gradient Descent: Running epoch 36\n",
      "Loss: 8.05559453491e-09                    \n",
      "\n",
      "Total training loss in this epoch: 5.51031814954\n",
      "Total testing loss in this epoch: 5.05108500524\n",
      "Stochastic Gradient Descent: Running epoch 37\n",
      "Loss: 1.03674190732e-08                    \n",
      "\n",
      "Total training loss in this epoch: 5.49960777355\n",
      "Total testing loss in this epoch: 4.99524774037\n",
      "Stochastic Gradient Descent: Running epoch 38\n",
      "Loss: 1.08110781222e-08                    \n",
      "\n",
      "Total training loss in this epoch: 5.36375821566\n",
      "Total testing loss in this epoch: 4.83433385879\n",
      "Stochastic Gradient Descent: Running epoch 39\n",
      "Loss: 1.70877959683e-08                    \n",
      "\n",
      "Total training loss in this epoch: 5.53180833495\n",
      "Total testing loss in this epoch: 4.9438840656\n",
      "Stochastic Gradient Descent: Running epoch 40\n",
      "Loss: 4.68808261662e-09                    \n",
      "\n",
      "Total training loss in this epoch: 5.03495727504\n",
      "Total testing loss in this epoch: 4.44883693196\n",
      "Stochastic Gradient Descent: Running epoch 41\n",
      "Loss: 1.76710893551e-08                    \n",
      "\n",
      "Total training loss in this epoch: 5.21199627164\n",
      "Total testing loss in this epoch: 4.59297822875\n",
      "Stochastic Gradient Descent: Running epoch 42\n",
      "Loss: 0.5                    \n",
      "\n",
      "Total training loss in this epoch: 5.10547587227\n",
      "Total testing loss in this epoch: 4.46562798512\n",
      "Stochastic Gradient Descent: Running epoch 43\n",
      "Loss: 0.5                    \n",
      "\n",
      "Total training loss in this epoch: 4.79565676155\n",
      "Total testing loss in this epoch: 4.12582637286\n",
      "Stochastic Gradient Descent: Running epoch 44\n",
      "Loss: 1.06618854303e-08                    \n",
      "\n",
      "Total training loss in this epoch: 4.91295569237\n",
      "Total testing loss in this epoch: 4.22689934109\n",
      "Stochastic Gradient Descent: Running epoch 45\n",
      "Loss: 4.77533477905e-09                    \n",
      "\n",
      "Total training loss in this epoch: 4.65868447391\n",
      "Total testing loss in this epoch: 3.96877209617\n",
      "Stochastic Gradient Descent: Running epoch 46\n",
      "Loss: 0.5                    \n",
      "\n",
      "Total training loss in this epoch: 4.44140037489\n",
      "Total testing loss in this epoch: 3.73007061186\n",
      "Stochastic Gradient Descent: Running epoch 47\n",
      "Loss: 9.49550884376e-09                    \n",
      "\n",
      "Total training loss in this epoch: 4.65696054072\n",
      "Total testing loss in this epoch: 3.93640540959\n",
      "Stochastic Gradient Descent: Running epoch 48\n",
      "Loss: 0.5                    \n",
      "\n",
      "Total training loss in this epoch: 4.35505417223\n",
      "Total testing loss in this epoch: 3.60204726052\n",
      "Stochastic Gradient Descent: Running epoch 49\n",
      "Loss: 0.5                    \n",
      "\n",
      "Total training loss in this epoch: 4.52221712433\n",
      "Total testing loss in this epoch: 3.77063635319\n"
     ]
    }
   ],
   "source": [
    "model = hw2.sgd(**params)\n",
    "# Total loss function value in an epoch is the sum of \n",
    "# loss functions corresponding to every example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Validate using Keras model\n",
    "######################################################################\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.2170     \n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.2120     \n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.2150     \n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.2180     \n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.2080     \n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.2240     \n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.2130     \n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.2050     \n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.2160     \n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.2020     \n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1790     \n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1700     \n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1570     \n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1360     \n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1240     \n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1260     \n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1150     \n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1200     \n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1150     \n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1080     \n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1080     \n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1160     \n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1120     \n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1020     \n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1220     \n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1090     \n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1090     \n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1110     \n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1150     \n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.1240     \n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.2160     \n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.4090     \n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.4150     \n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 1s - loss: 0.2500 - acc: 0.5000     \n",
      " 967/1000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "x_train     = params['train_data'][:,:-1]\n",
    "y_train     = np.array([hw2.label_to_ytrue(2, x) for x in params['train_data'][:, -1]])\n",
    "x_test      = params['test_data'][:,:-1]\n",
    "y_test      = np.array([hw2.label_to_ytrue(2, x) for x in params['test_data'][:, -1]])\n",
    "\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(3, activation='relu', use_bias = False, kernel_initializer='random_uniform', input_dim = 2))\n",
    "keras_model.add(Dense(3, activation='relu', use_bias = False, kernel_initializer='random_uniform'))\n",
    "keras_model.add(Dropout(1.0/3))\n",
    "keras_model.add(Dense(2, activation='softmax', use_bias = False, kernel_initializer='random_uniform'))\n",
    "sgd = SGD(lr=params['eta'])\n",
    "keras_model.compile(optimizer=sgd, loss='mean_squared_error', metrics=['accuracy'])\n",
    "keras_model.fit(x_train, y_train, batch_size = 1, epochs=params['maxiter'])\n",
    "score = keras_model.evaluate(x_test, y_test, batch_size=1)\n",
    "kw = keras_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = keras_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##############################################################\n",
      "Weights corresponding to best testing accuracy (custom SGD): \n",
      "##############################################################\n",
      "W1:  [[-1.94362051  3.93211241]\n",
      " [ 6.0522785  -3.14764066]\n",
      " [-2.55736844  1.33212314]]\n",
      "W2:  [[  2.57878635e+00  -1.22965297e+00   1.74857916e+00]\n",
      " [ -3.28531420e-01   6.56854888e+00  -7.38426162e-01]\n",
      " [  3.88404427e+00  -4.56413107e-03   2.07593749e+00]]\n",
      "W3:  [[-0.03987199  0.02878348 -0.03466736]\n",
      " [ 0.039872   -0.02878348  0.03466737]]\n"
     ]
    }
   ],
   "source": [
    "print \"\"\"\n",
    "##############################################################\n",
    "Weights corresponding to best testing accuracy (custom SGD): \n",
    "##############################################################\"\"\"\n",
    "print \"W1: \", model['w'][0]\n",
    "print \"W2: \", model['w'][1]\n",
    "print \"W3: \", model['w'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##############################################################\n",
      "Weights obtained from Keras: \n",
      "##############################################################\n",
      "Keras W1: [[ 0.00622058  0.0010522  -0.00709496]\n",
      " [ 0.03349522  0.04137478 -0.02858669]]\n",
      "Keras W2: [[-0.02256859  0.03819745  0.03867364]\n",
      " [-0.01596046 -0.03070686 -0.03310263]\n",
      " [ 0.02510785 -0.00401345  0.02000277]]\n",
      "Keras W3: [[ 0.00108399 -0.00402838]\n",
      " [ 0.00430376 -0.02744282]\n",
      " [ 0.04630783  0.04273371]]\n"
     ]
    }
   ],
   "source": [
    "print \"\"\"\n",
    "##############################################################\n",
    "Weights obtained from Keras: \n",
    "##############################################################\"\"\"\n",
    "print \"Keras W1:\", kw[0]\n",
    "print \"Keras W2:\", kw[1]\n",
    "print \"Keras W3:\", kw[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kwt = []\n",
    "kwt.append(kw[0].T)\n",
    "kwt.append(kw[1].T)\n",
    "kwt.append(kw[2].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# None of the units will drop out hence we use a 0 probability for dropout\n",
    "testing_mask = hw2.generate_masks_for_layers(3, [2, 3, 3], [0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####################################################################\n",
      "Precision, Recall and F-score for model output by running custom SGD\n",
      "#####################################################################\n",
      "\n",
      "Precision:  [ 1.          0.99800399]\n",
      "Recall:  [ 0.998  1.   ]\n",
      "F-Score:  [ 0.998999  0.999001]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find the f_score and other metrics\n",
    "precision_recall_fscore = hw2.get_precision_recall_fscore(\n",
    "    params['shuffle_order_testing'],\n",
    "    params['test_data'], model['w'],\n",
    "    params['z'],\n",
    "    params['a'],\n",
    "    params['y'],\n",
    "    testing_mask)\n",
    "print \"\"\"\n",
    "#####################################################################\n",
    "Precision, Recall and F-score for model output by running custom SGD\n",
    "#####################################################################\n",
    "\"\"\"\n",
    "print \"Precision: \", precision_recall_fscore[0]\n",
    "print \"Recall: \", precision_recall_fscore[1]\n",
    "print \"F-Score: \", precision_recall_fscore[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####################################################################\n",
      "Precision, Recall and F-score for model generated by running Keras SGD\n",
      "#####################################################################\n",
      "\n",
      "Precision:  [ 0.5  0. ]\n",
      "Recall:  [ 1.  0.]\n",
      "F-Score:  [ 0.66666667  0.        ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find the f_score and other metrics\n",
    "precision_recall_fscore = hw2.get_precision_recall_fscore(\n",
    "    params['shuffle_order_testing'],\n",
    "    params['test_data'],\n",
    "    kwt,\n",
    "    params['z'],\n",
    "    params['a'],\n",
    "    params['y'], \n",
    "    testing_mask)\n",
    "print \"\"\"\n",
    "#####################################################################\n",
    "Precision, Recall and F-score for model generated by running Keras SGD\n",
    "#####################################################################\n",
    "\"\"\"\n",
    "print \"Precision: \", precision_recall_fscore[0]\n",
    "print \"Recall: \", precision_recall_fscore[1]\n",
    "print \"F-Score: \", precision_recall_fscore[2]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
